{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def transpose(m):\n",
    "    return [[m[j][i] for j in range(len(m))] for i in range(len(m[0]))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(2, 2), stride=1, padding=0):\n",
    "\n",
    "        # Set the input and output channels\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Set the kernel size, stride, and padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initialize weights with torch\n",
    "        self.torch_filter = torch.randn(out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
    "\n",
    "        # Initialize the weights manually\n",
    "        self.filter = [[[[x+kernel_size[0]*y for x in range(1,kernel_size[0]+1)] for y in range(0, kernel_size[1])] for _ in range(0, in_channels)] for l in range(0,out_channels)]\n",
    "        self.filter = torch.tensor(self.filter, dtype=torch.float64)\n",
    "\n",
    "        # Assertions\n",
    "        assert len(self.filter) == self.torch_filter.shape[0]\n",
    "        assert len(self.filter[0]) == self.torch_filter.shape[1]\n",
    "        assert len(self.filter[0][0]) == self.torch_filter.shape[2]\n",
    "        assert len(self.filter[0][0][0]) == self.torch_filter.shape[3]\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        b, c, h, w = input_batch.size()\n",
    "\n",
    "        # Define output shape\n",
    "        output_height = int((h - self.kernel_size[0] + 2 * self.padding)/ self.stride + 1)\n",
    "        output_width = int((w - self.kernel_size[1] + 2 * self.padding)/ self.stride + 1)\n",
    "\n",
    "        # Initialize output with torch\n",
    "        output_tensor_torch = torch.zeros(b, self.out_channels, output_height, output_width)\n",
    "\n",
    "        # Initialize output with zeros\n",
    "        output_tensor = [[[[0 for x in range(output_width)] for y in range(output_height)] for o in range(0, self.out_channels)] for image in range(0, b)]\n",
    "        output_tensor = torch.tensor(output_tensor, dtype=torch.float64)\n",
    "\n",
    "        # Assertions\n",
    "        assert len(output_tensor) == output_tensor_torch.shape[0]\n",
    "        assert len(output_tensor[0]) == output_tensor_torch.shape[1]\n",
    "        assert len(output_tensor[0][0]) == output_tensor_torch.shape[2]\n",
    "        assert len(output_tensor[0][0][0]) == output_tensor_torch.shape[3]\n",
    "\n",
    "        # Unfold Input\n",
    "        unfolded = F.unfold(input_batch, (2,2))\n",
    "\n",
    "        # Reshape Filter\n",
    "        filter_reshaped = self.filter.view(self.filter.size(0), -1).T\n",
    "\n",
    "        # Perform Convolution\n",
    "        conv_output = unfolded.transpose(1,2).matmul(filter_reshaped).transpose(1,2)\n",
    "\n",
    "        # Fold Back\n",
    "        out = F.fold(conv_output, output_tensor.shape[2:4], (1,1))\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape = torch.Size([16, 3, 32, 32])\n",
      "Filter Shape = torch.Size([2, 3, 2, 2])\n",
      "Output Shape B4 = torch.Size([16, 2, 31, 31])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "output_channels = 2 # Also the number of Filters\n",
    "batch_size = 16 # How many examples (images) per batch\n",
    "color_channels = 3 # The depth of a single example (RGB)\n",
    "input_width = 32 # Width of an image\n",
    "input_height = 32 # Height of an image\n",
    "\n",
    "# Initialize Network\n",
    "\n",
    "# Initialize input randomly\n",
    "# input_batch = torch.randn(4, 3, 3, 3) # (Batch Size, Input Channels, Input Height, Input Width)\n",
    "\n",
    "# Initialize input simplistic example\n",
    "input_batch = [[[[x+input_width*y for x in range(1,input_width+1)] for y in range(0, input_height)] for _ in range(0, color_channels)] for b in range(batch_size)]\n",
    "input_batch = torch.tensor(input_batch, dtype=torch.float64)\n",
    "\n",
    "\n",
    "conv = Conv2D(in_channels=input_batch.shape[1], out_channels=output_channels)\n",
    "\n",
    "# Start Forward Pass\n",
    "output_batch = conv(input_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}