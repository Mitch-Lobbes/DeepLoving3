{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[  2.5842,  -0.1989,  -4.9084,  ...,   0.4085,  -1.1752,  -2.4602],\n          [ -3.7182,   3.7164,   6.1937,  ...,   1.2530,   0.4965,  -1.0982],\n          [ -0.3931,  -1.2821,   0.7987,  ...,   0.0476,   4.4236,   1.9649],\n          ...,\n          [ -5.7531,  -1.4558,   3.8175,  ...,  -2.3677,   1.2201,   0.9530],\n          [  1.0636,  -0.9391,   4.5892,  ...,  -1.3669,  -0.1630,   0.5820],\n          [ -1.8402,  -2.0190,  -0.8690,  ...,  -2.2354,  -2.5158,   4.1031]],\n\n         [[ -3.4498,  -3.7207,  -5.3942,  ...,   4.5733,   4.6192,  -3.0451],\n          [ -1.7342,   4.7661,  -2.8703,  ...,  -3.2667,  -5.5867,   4.5160],\n          [ -0.1363,   5.7277,   7.4695,  ...,  -4.5361, -11.3935, -10.6704],\n          ...,\n          [  1.9613,   0.8342,  -5.4449,  ...,  -1.5805,  -5.8310,   0.1576],\n          [  0.9721,  -0.6311,   4.0244,  ...,  -2.9763,   3.2695,  -2.6310],\n          [  5.3097,  -1.3595,   5.0795,  ...,   9.1282,   7.6029,   5.1334]]],\n\n\n        [[[ -3.6233,   0.2643,  -2.5621,  ...,   1.5642,   6.6145,   3.2811],\n          [  0.1113,  -0.5177,   0.1246,  ...,   2.2375,  -2.2094,  -2.6724],\n          [ -0.9323,  -1.9690,  -0.1980,  ...,  -0.0138,  -2.8867,   0.3423],\n          ...,\n          [  0.5533,   0.8618,   1.0418,  ...,   3.2166,   0.0983,  -4.2352],\n          [  0.0953,  -0.4980,   0.3115,  ...,   7.0049,   3.4971,   1.1781],\n          [  0.0748,   1.7897,   1.0655,  ...,  -1.2587,   1.2769,   0.8473]],\n\n         [[ -0.3526,   1.3319,  -4.8938,  ...,  -5.2186, -11.0607,  -8.3534],\n          [  1.4670,   4.6992,  -2.1689,  ...,   9.4533,  -7.9084,  -1.5799],\n          [  1.4443,  -3.4237,   3.6041,  ...,   9.0231,   0.5864,   2.7044],\n          ...,\n          [ 13.7042,   1.0355,  -6.8730,  ...,  -5.2567,  -2.0658,   3.9758],\n          [  5.4773,   6.7204, -10.0316,  ...,   3.0623,   3.1143,  -3.1439],\n          [ -0.2998,  -4.0278,  -6.4517,  ...,   5.5752,   5.7617,  -1.7283]]],\n\n\n        [[[ -0.4148,   0.1823,  -2.4713,  ...,  -1.3494,  -4.0925,   0.2032],\n          [  1.2731,   0.6892,   0.4798,  ...,   1.3478,   2.4490,  -2.2345],\n          [  2.7522,   2.2997,  -1.5004,  ...,  -2.7428,   1.0680,   1.0700],\n          ...,\n          [ -0.5102,  -0.6176,  -1.7305,  ...,  -2.0061,   1.9024,   0.0885],\n          [ -1.0658,   0.7539,  -1.0196,  ...,   1.4842,  -0.1685,  -1.3247],\n          [ -2.6977,   0.9073,   0.7385,  ...,  -2.0885,   9.0407,   2.3961]],\n\n         [[ -1.8158,  -8.0184,   3.0740,  ...,   0.2085,  -0.0228,   3.3916],\n          [ -0.8041,  -0.4315,  -2.9148,  ...,  -0.6002,  -5.1911,   1.1184],\n          [  6.1502,  -3.7911,   2.8541,  ...,  -1.7393,   3.4043,  -3.4126],\n          ...,\n          [  8.4076,   3.9196,  -1.3040,  ...,  -4.3906,   5.5214,  -1.9091],\n          [  4.3290,   4.0203,  -6.0624,  ...,  -1.6571,   2.6291,   0.8422],\n          [  1.4354,  -1.3533,  -4.6196,  ...,  -0.2581,  -1.3279,  -2.3436]]],\n\n\n        ...,\n\n\n        [[[  4.1988,   3.9069,   3.5948,  ...,   0.2746,  -3.6436,   0.4623],\n          [  0.6449,   0.3804,   0.9386,  ...,   0.9752,   2.4274,   2.9031],\n          [ -1.7109,  -7.8081,  -5.1235,  ...,   6.0242,   1.8091,  -2.6775],\n          ...,\n          [ -1.8779,  -4.0763,  -1.8163,  ...,  -0.3757,  -0.3367,   1.7951],\n          [  0.6644,   3.7326,   2.1970,  ...,  -2.7908,   0.9174,   5.4199],\n          [ -4.3621,   2.5879,   2.8533,  ...,  -1.0213,  -3.3750,  -0.2455]],\n\n         [[ -1.3687,  -5.5300,  -5.6961,  ...,   3.5294,  -6.5286,  -4.1725],\n          [  6.8731,   6.4920,  -0.7465,  ...,  -0.9382,  -8.2034,  -0.6028],\n          [ -3.8232,   9.7314,   1.5572,  ...,  -6.7616,  -4.3355,   7.0669],\n          ...,\n          [  1.4947,   3.1495,   2.2699,  ...,  -1.1963,   1.6920,   2.6963],\n          [  1.4098,  -4.1103,  -1.5988,  ...,  -0.4231,   5.2143,   5.8833],\n          [  1.6986,  -1.2077,   1.1073,  ...,  -5.8433,   0.7497,   8.2796]]],\n\n\n        [[[  3.7340,   0.1221,   2.7614,  ...,  -3.6922,  -1.5545,  -2.6443],\n          [  3.7485,   3.1058,   1.3394,  ...,   0.3820,   1.7530,   0.7013],\n          [ -2.2453,   1.4560,   2.6476,  ...,   1.1441,  -1.3503,  -0.8274],\n          ...,\n          [ -2.9597,   4.0831,   2.1225,  ...,   1.6620,  -0.7366,   2.9949],\n          [ -0.7152,  -3.1798,  -0.3055,  ...,  -1.1415,  -0.8004,  -2.1517],\n          [  1.6712,  -0.8303,   1.1888,  ...,  -0.1664,   0.2472,   2.2046]],\n\n         [[  1.3860,  -7.9880,  -6.0470,  ...,   6.1988,   0.7886,   1.7389],\n          [  5.7875,  -0.9375,  -2.8259,  ...,   3.3906,   3.7211,   3.6240],\n          [  4.2229,  11.1146,   4.3217,  ...,  -2.7308,   7.9930,   5.2592],\n          ...,\n          [-13.3211,   5.4422,  -8.5652,  ...,  -1.3453,   5.3382,   1.3931],\n          [ -2.5121,   0.3305,  -2.0707,  ...,  -2.4542,   4.7478,  -1.2292],\n          [ -1.6486,  -2.0893,  -0.7793,  ...,  -3.7430,   3.0440,  -0.8415]]],\n\n\n        [[[ -3.0892,  -0.7265,  -1.0276,  ...,  -4.8617,   1.5236,   1.0689],\n          [ -0.9023,  -3.7864,  -2.0156,  ...,  -1.3632,   0.0240,  -2.3735],\n          [ -3.0224,   0.0862,  -0.3066,  ...,   2.6140,   0.4269,   1.0155],\n          ...,\n          [  0.5039,   0.5007,  -1.3969,  ...,   3.3718,   3.4886,   4.3938],\n          [  2.8869,   4.1770,   0.7903,  ...,   1.6148,   3.1322,   1.4585],\n          [ -0.2082,  -0.8329,   2.6466,  ...,   0.5698,   2.5818,  -3.0550]],\n\n         [[ -3.7101,   2.0559,   0.2051,  ...,  -6.6640,   1.0769,  -2.8113],\n          [ -0.6607,   2.6446,  -0.0343,  ...,  -6.0824,  -0.7556,  -1.9510],\n          [  4.6197,   2.6638,  -0.7635,  ...,  -1.5426,  -0.2157,  -6.9507],\n          ...,\n          [ -2.4747,  -7.1355,   1.7810,  ...,   5.7291,   1.1263,  -2.8659],\n          [ -1.6606,  -8.5007,  -0.7212,  ...,  -1.6234,  -1.3912,   4.0486],\n          [  0.3893,   1.6703,  -0.6388,  ...,   0.5106,  -4.7613,  -1.0480]]]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv2DFunc(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward\n",
    "    passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "\n",
    "        # store objects for the backward\n",
    "        ctx.save_for_backward(input_batch)\n",
    "        ctx.save_for_backward(kernel)\n",
    "\n",
    "        # get shape of input tensor\n",
    "        b, c, H, W = input_batch.size()\n",
    "\n",
    "        # calculate output shape\n",
    "        oH = (H - kernel.size(2)) // stride + 1\n",
    "        oW = (W - kernel.size(3)) // stride + 1\n",
    "\n",
    "        # create output tensor\n",
    "        output_batch = torch.zeros(b, kernel.size(0), oH, oW)\n",
    "\n",
    "        # Unfold Input\n",
    "        unfolded = F.unfold(input_batch, (2,2))\n",
    "\n",
    "        # Reshape Filter\n",
    "        filter_reshaped = kernel.view(kernel.size(0), -1).T\n",
    "\n",
    "        # Perform Convolution\n",
    "        conv_output = unfolded.transpose(1,2).matmul(filter_reshaped).transpose(1,2)\n",
    "\n",
    "        # Fold Back\n",
    "        output_batch = F.fold(conv_output, output_batch.shape[2:4], (1,1))\n",
    "\n",
    "        return output_batch\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the\n",
    "        gradient of the loss with respect to the output, and we need\n",
    "        to compute the gradient of the loss with respect to the\n",
    "        input\n",
    "        \"\"\"\n",
    "\n",
    "        # retrieve stored objects\n",
    "        input_batch, kernel = ctx.saved_tensors\n",
    "\n",
    "        # your code here\n",
    "        input_batch_grad = F.fold(F.unfold(grad_output, kernel.size()[2:]).matmul(kernel.flatten()), input_batch.shape[2:],(1,1))\n",
    "        kernel_grad = F.fold(F.unfold(input_batch, grad_output.size()[2:]).matmul(grad_output.flatten()),kernel.shape[2:],(1,1))\n",
    "\n",
    "        # The gradients of the inputs. For anything that doesn't have # a gradient (the stride and padding) you can\n",
    "        # return None.\n",
    "\n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "\n",
    "\n",
    "input_batch = torch.randn(16,3,32,32)\n",
    "kernel = torch.randn(2, 3, 2, 2)\n",
    "Conv2DFunc.apply(input_batch, kernel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}